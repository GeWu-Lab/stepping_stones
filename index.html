<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="ECCV">
  <meta property="og:title" content="ECCV2024"/>
  <meta property="og:description" content="Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Stepping Stones</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/ucasmjc" target="_blank">Juncheng Ma</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Peiwen Sun</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yaoting Wang</a><sup>3</sup>,
                  </span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Di Hu</a><sup>3*</sup>,
                  </span>
                  
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>University of Chinese Academy of Sciences,
                      <br><sup>2</sup>Beijing University of Posts and Telecommunications,
                      <br><sup>3</sup>Gaoling School of Artificial Intelligence, Renmin University of China, Beijing<br><strong>ECCV 2024</strong></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author.</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="static/pdfs/09290.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/09290-supp.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/GeWu-Lab/Stepping-Stones" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="item">
        <img src="static/images/teaser.png" alt="MY ALT TEXT"/>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Audio-Visual Segmentation (AVS) aims to achieve pixel-level localization of sound sources in videos, while Audio-Visual Semantic Segmentation (AVSS), as an extension of AVS, further pursues semantic understanding of audio-visual scenes. However, since the AVSS task requires the establishment of audio-visual correspondence and semantic understanding simultaneously, we observe that previous methods have struggled to handle this mashup of objectives in end-to-end training, resulting in insufficient learning and sub-optimization. Therefore, we propose a two-stage training strategy called Stepping Stones, which decomposes the AVSS task into two simple subtasks from localization to semantic understanding, which are fully optimized in each stage to achieve step-by-step global optimization. This training strategy has also proved its generalization and effectiveness on existing methods. To further improve the performance of AVS tasks, we propose a novel framework Adaptive Audio Visual Segmentation, in which we incorporate an adaptive audio query generator and integrate masked attention into the transformer decoder, facilitating the adaptive fusion of visual and audio features.  Extensive experiments demonstrate that our methods achieve state-of-the-art results on all three AVS benchmarks. The code is publicly available at https://ss-aavs.github.io/.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="hero teaser">
  <div class="container is-max-desktop">
      <div class="hero-body">
          <h2 class="title is-3 has-text-centered">Method</h2>
          <img src="static/images/model.png" alt="MY ALT TEXT"/>
          <br><br>
          <h2 class="subtitle has-text-justified">
              <p> To solve this problem and boost the performance of previous AVSS methods, we propose a simple yet effective two-stage progressive training strategy called Stepping Stones to decompose the intricate AVSS task into two relatively simple subtasks to be fully learned in the two stages. In the first stage, the model is trained using binary labels without semantic information, prioritizing the alignment of audio and visual modalities to establish fine-grained correlations at the pixel level.  Subsequently, in the second stage, the model is trained with semantic labels as supervision, leveraging the sound source localization results from the first stage as stepping stones. These stepping stones serve as a shortcut for the model to readily recognize sounding pixels during training, thus focusing on the semantic understanding of the sound source. This approach is generalizable and can be readily applied to existing end-to-end AVSS models. Moreover, acknowledging the considerable error between the first stage results and ground truth during inference, we also introduce the Robust Audio-aware Key Generator to robustly integrate auditory information into visual features as keys in the cross-modal cross-attention module. Since the second stage leverages the sound source localization results as prior knowledge, the model can focus solely on the semantic comprehension of a given sound source region, thus overcoming the challenge of ambiguous supervision signals during end-to-end learning. Furthermore, we comprehensively train audio-visual alignment and visually dominant semantic understanding in two separate training phases, which facilitates the optimization of both subtasks of the model, leading to improved global optimization.
              </p>
          </h2>
      </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
      <div class="hero-body">
          <h2 class="title is-3 has-text-centered">Quantitative Comparision</h2>
          <img src="static/images/results.png" alt="MY ALT TEXT"/>
          <br><br>
          <h2 class="subtitle has-text-justified">
              <p> We conduct a comprehensive comparison between our AAVS model and existing methods on the AVSBench dataset. Our AAVS model outperforms previous methods in terms of mIoU and F-score for both the S4 and MS3 subtasks. Specifically, for the S4 subtask, AAVS demonstrates performance improvements of 1.12% in mIoU and 0.93% in the F-score. For the MS3 subtask, AAVS showcases performance improvements of 2.50% in mIoU and 0.23% in the F-score. We attribute these enhancements to the efficacy of our proposed method, which enhances audio-visual modal alignment through adaptive fusion of visual and audio features. It is noteworthy that the improvement on the MS3 subtask is significantly higher than that on the S4 subtask, which we attribute to the greater complexity of the MS3 task scenarios and the adaptive nature of our approach in addressing multi-sound source localization challenges. For the AVSS subtask, we also conduct experiments to compare our approach with previous work. As depicted in the table, the AAVS model alone demonstrates performance enhancements of 4.62% mIoU and 4.33% F-score. Moreover, when the Stepping Stones training strategy is applied to the AAVS model, performance is further substantially improved by 7.22% mIoU and 6.87% F-score. This significant improvement arises partly from the capabilities of the AAVS model itself and partly from the \textit{Stepping Stones} training strategy, which aids the model in learning audiovisual alignment and semantic understanding more effectively.
              </p>
          </h2>
      </div>
  </div>
</section>
<section class="hero is-small">
  <div class="hero-body">
    <h2 class="title is-3 has-text-centered">Qualitative Comparision</h2>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/s4.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison with previous methods on S4 subtask.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/ms3.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison with previous methods on MS3 subtask.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/v2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison with previous methods on AVSS subtask.
       </h2>
     </div>
  </div>
</div>
</div>
</section>

<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/09290.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
